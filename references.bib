@article{Pigeon,
  title        = {Pigeons ({{Columba}} Livia) as {{Trainable Observers}} of {{Pathology}} and {{Radiology Breast Cancer Images}}},
  author       = {Levenson, Richard M. and Krupinski, Elizabeth A. and Navarro, Victor M. and Wasserman, Edward A.},
  editor       = {Coles, Jonathan A},
  date         = {2015-11-18},
  journaltitle = {PLOS ONE},
  shortjournal = {PLoS ONE},
  volume       = {10},
  number       = {11},
  pages        = {e0141357},
  issn         = {1932-6203},
  doi          = {10.1371/journal.pone.0141357},
  url          = {https://dx.plos.org/10.1371/journal.pone.0141357},
  urldate      = {2025-06-26},
  abstract     = {Pathologists and radiologists spend years acquiring and refining their medically essential visual skills, so it is of considerable interest to understand how this process actually unfolds and what image features and properties are critical for accurate diagnostic performance. Key insights into human behavioral tasks can often be obtained by using appropriate animal models. We report here that pigeons (Columba livia)—which share many visual system properties with humans—can serve as promising surrogate observers of medical images, a capability not previously documented. The birds proved to have a remarkable ability to distinguish benign from malignant human breast histopathology after training with differential food reinforcement; even more importantly, the pigeons were able to generalize what they had learned when confronted with novel image sets. The birds’ histological accuracy, like that of humans, was modestly affected by the presence or absence of color as well as by degrees of image compression, but these impacts could be ameliorated with further training. Turning to radiology, the birds proved to be similarly capable of detecting cancer-relevant microcalcifications on mammogram images. However, when given a different (and for humans quite difficult) task—namely, classification of suspicious mammographic densities (masses)—the pigeons proved to be capable only of image memorization and were unable to successfully generalize when shown novel examples. The birds’ successes and difficulties suggest that pigeons are well-suited to help us better understand human medical image perception, and may also prove useful in performance assessment and development of medical imaging hardware, image processing, and image analysis tools.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/3RDRLKLL/Levenson et al. - 2015 - Pigeons (Columba livia) as Trainable Observers of .pdf}
}

@article{ChatGPT,
  title        = {Performance of {{ChatGPT}} on {{USMLE}}: {{Potential}} for {{AI-assisted}} Medical Education Using Large Language Models},
  shorttitle   = {Performance of {{ChatGPT}} on {{USMLE}}},
  author       = {Kung, Tiffany H. and Cheatham, Morgan and Medenilla, Arielle and Sillos, Czarina and De Leon, Lorie and Elepaño, Camille and Madriaga, Maria and Aggabao, Rimel and Diaz-Candido, Giezel and Maningo, James and Tseng, Victor},
  editor       = {Dagan, Alon},
  date         = {2023-02-09},
  journaltitle = {PLOS Digital Health},
  shortjournal = {PLOS Digit Health},
  volume       = {2},
  number       = {2},
  pages        = {e0000198},
  issn         = {2767-3170},
  doi          = {10.1371/journal.pdig.0000198},
  url          = {https://dx.plos.org/10.1371/journal.pdig.0000198},
  urldate      = {2025-06-28},
  abstract     = {We evaluated the performance of a large language model called ChatGPT on the United States Medical Licensing Exam (USMLE), which consists of three exams: Step 1, Step 2CK, and Step 3. ChatGPT performed at or near the passing threshold for all three exams without any specialized training or reinforcement. Additionally, ChatGPT demonstrated a high level of concordance and insight in its explanations. These results suggest that large language models may have the potential to assist with medical education, and potentially, clinical decision-making.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/KQJQ8S62/Kung et al. - 2023 - Performance of ChatGPT on USMLE Potential for AI-.pdf}
}

@article{LLM,
  title        = {A Large Language Model for Electronic Health Records},
  author       = {Yang, Xi and Chen, Aokun and PourNejatian, Nima and Shin, Hoo Chang and Smith, Kaleb E. and Parisien, Christopher and Compas, Colin and Martin, Cheryl and Costa, Anthony B. and Flores, Mona G. and Zhang, Ying and Magoc, Tanja and Harle, Christopher A. and Lipori, Gloria and Mitchell, Duane A. and Hogan, William R. and Shenkman, Elizabeth A. and Bian, Jiang and Wu, Yonghui},
  date         = {2022-12-26},
  journaltitle = {npj Digital Medicine},
  shortjournal = {npj Digit. Med.},
  volume       = {5},
  number       = {1},
  pages        = {194},
  issn         = {2398-6352},
  doi          = {10.1038/s41746-022-00742-2},
  url          = {https://www.nature.com/articles/s41746-022-00742-2},
  urldate      = {2025-06-28},
  abstract     = {Abstract                            There is an increasing interest in developing artificial intelligence (AI) systems to process and interpret electronic health records (EHRs). Natural language processing (NLP) powered by pretrained language models is the key technology for medical AI systems utilizing clinical narratives. However, there are few clinical language models, the largest of which trained in the clinical domain is comparatively small at 110 million parameters (compared with billions of parameters in the general domain). It is not clear how large clinical language models with billions of parameters can help medical AI systems utilize unstructured EHRs. In this study, we develop from scratch a large clinical language model—GatorTron—using {$>$}90 billion words of text (including {$>$}82 billion words of de-identified clinical text) and systematically evaluate it on five clinical NLP tasks including clinical concept extraction, medical relation extraction, semantic textual similarity, natural language inference (NLI), and medical question answering (MQA). We examine how (1) scaling up the number of parameters and (2) scaling up the size of the training data could benefit these NLP tasks. GatorTron models scale up the clinical language model from 110 million to 8.9 billion parameters and improve five clinical NLP tasks (e.g., 9.6\% and 9.5\% improvement in accuracy for NLI and MQA), which can be applied to medical AI systems to improve healthcare delivery. The GatorTron models are publicly available at:               https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron\_og               .},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/JX6UDU4Q/Yang et al. - 2022 - A large language model for electronic health recor.pdf}
}

@article{LLM2,
  title        = {Large Language Models Encode Clinical Knowledge},
  author       = {Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S. Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and Payne, Perry and Seneviratne, Martin and Gamble, Paul and Kelly, Chris and Babiker, Abubakr and Schärli, Nathanael and Chowdhery, Aakanksha and Mansfield, Philip and Demner-Fushman, Dina and Agüera Y Arcas, Blaise and Webster, Dale and Corrado, Greg S. and Matias, Yossi and Chou, Katherine and Gottweis, Juraj and Tomasev, Nenad and Liu, Yun and Rajkomar, Alvin and Barral, Joelle and Semturs, Christopher and Karthikesalingam, Alan and Natarajan, Vivek},
  date         = {2023-08-03},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume       = {620},
  number       = {7972},
  pages        = {172--180},
  issn         = {0028-0836, 1476-4687},
  doi          = {10.1038/s41586-023-06291-2},
  url          = {https://www.nature.com/articles/s41586-023-06291-2},
  urldate      = {2025-06-28},
  abstract     = {Abstract                            Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and~a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension,~reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model               1               (PaLM,~a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM               2               on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA               3               , MedMCQA               4               , PubMedQA               5               and Measuring Massive Multitask Language Understanding (MMLU) clinical topics               6               ), including 67.6\% accuracy on MedQA~(US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17\%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/8VTZ9JXM/Singhal et al. - 2023 - Large language models encode clinical knowledge.pdf}
}

@online{MedPalm,
  title        = {Med-{{PaLM}}: {{A Medical Large Language Model}} - {{Google Research}}},
  shorttitle   = {Med-{{PaLM}}},
  url          = {https://sites.research.google/med-palm/},
  urldate      = {2025-06-28},
  abstract     = {Discover Med-PaLM, a large language model designed for medical purposes. See how we developed our AI system to accurately answer medical questions.},
  langid       = {english},
  organization = {Med-PaLM: A Medical Large Language Model - Google Research},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/HWT5U7GU/med-palm.html}
}

@article{Survey,
  title    = {A {{Survey}} of {{Large Language Models}} in {{Medicine}}: {{Progress}}, {{Application}}, and {{Challenge}}},
  author   = {Zhou, Hongjian and Liu, Fenglin and Gu, Boyang and Zou, Xinyu and Huang, Jinfa and Wu, Jinge and Li, Yiru and Chen, Sam and Zhou, Peilin and Liu, Junling and Hua, Yining and Mao, Chengfeng and Wu, Xian and Zheng, Yefeng and Clifton, Lei and Li, Zheng and Luo, Jiebo and Clifton, David},
  abstract = {Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. While there has been a burgeoning trend in research focusing on the employment of LLMs in supporting different medical tasks (e.g., enhancing clinical diagnostics and providing medical education), a comprehensive review of these efforts, particularly their development, practical applications, and outcomes in medicine, remains scarce. Therefore, this review aims to provide a detailed overview of the development and deployment of LLMs in medicine, including the challenges and opportunities they face. In terms of development, we provide a detailed introduction to the principles of existing medical LLMs, including their basic model structures, number of parameters, and sources and scales of data used for model development. It serves as a guide for practitioners in developing medical LLMs tailored to their specific needs. In terms of deployment, we offer a comparison of the performance of different LLMs across various medical tasks, and further compare them with state-of-the-art lightweight models, aiming to provide a clear understanding of the distinct advantages and limitations of LLMs in medicine. Overall, in this review, we address the following study questions: 1) What are the practices for developing medical LLMs? 2) How to measure the medical task performance of LLMs in a medical setting? 3) How have medical LLMs been employed in real-world practice? 4) What challenges arise from the use of medical LLMs? and 5) How to more effectively develop and deploy medical LLMs? By answering these questions, this review aims to provide insights into the opportunities and challenges of LLMs in medicine and serve as a practical resource for constructing effective medical LLMs. We also maintain a regularly updated list of practical guides on medical LLMs at: https://github.com/AI-in-Health/MedLLMsPracticalGuide.},
  langid   = {english},
  file     = {/Users/choekyelnyungmartsang/Zotero/storage/AGYVN4NV/Zhou et al. - A Survey of Large Language Models in Medicine Pro.pdf}
}

@online{AINotReady2024,
  title   = {{{AI}} Not {{Ready}} to {{Perform Basic Medical Coding}} | {{Norwood}}},
  date    = {2024-04-26},
  url     = {https://www.norwood.com/nejm-study-ai-not-ready-to-perform-basic-medical-coding-let-alone-replace-people/},
  urldate = {2025-06-28},
  file    = {/Users/choekyelnyungmartsang/Zotero/storage/VLSMTBNF/nejm-study-ai-not-ready-to-perform-basic-medical-coding-let-alone-replace-people.html}
}

@article{Apple,
  title    = {The {{Illusion}} of {{Thinking}}: {{Understanding}} the {{Strengths}} and {{Limitations}} of {{Reasoning Models}} via the {{Lens}} of {{Problem Complexity}}},
  author   = {Shojaee, Parshin and Mirzadeh, Iman and Alizadeh, Keivan and Horton, Maxwell and Bengio, Samy and Farajtabar, Mehrdad},
  abstract = {Recent generations of frontier language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established mathematical and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from data contamination and does not provide insights into the reasoning traces’ structure and quality. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of compositional complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs “think”. Through extensive experimentation across diverse puzzles, we show that frontier LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having an adequate token budget. By comparing LRMs with their standard LLM counterparts under equivalent inference compute, we identify three performance regimes: (1) lowcomplexity tasks where standard models surprisingly outperform LRMs, (2) medium-complexity tasks where additional thinking in LRMs demonstrates advantage, and (3) high-complexity tasks where both models experience complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across puzzles. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models’ computational behavior, shedding light on their strengths, limitations, and ultimately raising crucial questions about their true reasoning capabilities.},
  langid   = {english},
  file     = {/Users/choekyelnyungmartsang/Zotero/storage/UH6FCJZC/Shojaee et al. - The Illusion of Thinking Understanding the Streng.pdf}
}

@article{LLM3,
  title        = {Large {{Language Models Are Poor Medical Coders}} — {{Benchmarking}} of {{Medical Code Querying}}},
  author       = {Soroush, Ali and Glicksberg, Benjamin S. and Zimlichman, Eyal and Barash, Yiftach and Freeman, Robert and Charney, Alexander W. and Nadkarni, Girish N and Klang, Eyal},
  date         = {2024-04-25},
  journaltitle = {NEJM AI},
  shortjournal = {NEJM AI},
  volume       = {1},
  number       = {5},
  issn         = {2836-9386},
  doi          = {10.1056/AIdbp2300040},
  url          = {https://ai.nejm.org/doi/10.1056/AIdbp2300040},
  urldate      = {2025-06-28},
  abstract     = {BACKGROUND Large language models (LLMs) have attracted significant interest for automated clinical coding. However, early data show that LLMs are highly error-prone when mapping medical codes. We sought to quantify and benchmark LLM medical code querying errors across several available LLMs. METHODS We evaluated GPT-3.5, GPT-4, Gemini Pro, and Llama2-70b Chat performance and error patterns when querying medical billing codes. We extracted 12 months of unique International Classification of Diseases, 9th edition, Clinical Modification (ICD-9-CM), International Classification of Diseases, 10th edition, Clinical Modification (ICD-10-CM), and Current Procedural Terminology (CPT) codes from the Mount Sinai Health System electronic health record (EHR). Each LLM was provided with a code description and prompted to generate a billing code. Exact match accuracy and other performance metrics were calculated. Nonexact matches were analyzed using descriptive metrics and standardized measures of text and code similarity, including METEOR score, BERTScore, and cui2vec cosine similarity. We created and applied a CodeSTS manual similarity grading system to 200 randomly selected codes weighted by EHR code frequency. Using CodeSTS scores, we identified correct “equivalent” or “generalized” generated codes. RESULTS A total of 7697 ICD-9-CM, 15,950 ICD-10-CM, and 3673 CPT codes were extracted. GPT-4 had the highest exact match rate (ICD-9-CM: 45.9\%; ICD-10-CM: 33.9\%; CPT: 49.8\%). Among incorrectly matched codes, GPT-4 generated the most equivalent codes (ICD-9-CM: 7.0\%; ICD-10-CM: 10.9\%), and GPT-3.5 generated the most generalized but correct codes (ICD-9-CM: 29.9\%; ICD-10-CM: 18.5\%). Extracted code frequency, shorter codes, and shorter code descriptions were associated (P{$<$}0.05) with higher exact match rates in nearly all analyses. CONCLUSIONS All tested LLMs performed poorly on medical code querying, often generating codes conveying imprecise or fabricated information. LLMs are not appropriate},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/ADKEZJ6V/Soroush et al. - 2024 - Large Language Models Are Poor Medical Coders — Be.pdf}
}

@article{Privacy,
  title        = {De-Identification Is Not Enough: A Comparison between de-Identified and Synthetic Clinical Notes},
  shorttitle   = {De-Identification Is Not Enough},
  author       = {Sarkar, Atiquer Rahman and Chuang, Yao-Shun and Mohammed, Noman and Jiang, Xiaoqian},
  date         = {2024-11-29},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume       = {14},
  number       = {1},
  pages        = {29669},
  issn         = {2045-2322},
  doi          = {10.1038/s41598-024-81170-y},
  url          = {https://www.nature.com/articles/s41598-024-81170-y},
  urldate      = {2025-06-28},
  abstract     = {For synthetic note generation, we utilized the HIPAA-compliant Azure OpenAI platform provided by UTHealth in order to adhere to MIMIC-III’s data usage agreement requirement. Following the benchmark work for the utility aspect (ref. Table 2 in Vu et al.33), we limit ourselves to the discharge summary clinical notes and focus on the 50 most frequent ICD-9 codes as the classification targets. To assess the performance of the synthetic notes, micro- and macro-averaged values for AUC, precision, recall, and F1-score of the ICD-9 classifier were reported. Each pair of (clinical note, ICD-code) is treated as a distinct prediction to calculate the micro-averaged values. On the other hand, macro-averaged values are determined by computing metrics for each label and then averaging them. Classification precisions for top-k ICD-9 codes were also reported (k=1, 5, 8, 10, and 15). We mounted the membership inference attack as a privacy measurement technique (using the TensorFlow Privacy library34 released by Google; the details are discussed in the “Methods” section). We used three attack models (membership inference classifiers) on real data: the K-nearest neighbor model, the multilayer perceptron model, and the random forest model. Similar to11, the random forest attack model achieved the highest attacker advantage. We used the results from the random forest attacker model for subsequent reporting. To assess the performance of the membership inference attack, we report the attacker’s advantage (true\_positive\_rate(T P R) − f alse\_positive\_rate(F P R)) and the AUC.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/9ME2F62W/Sarkar et al. - 2024 - De-identification is not enough a comparison betw.pdf}
}

@article{Race,
  title        = {Large Language Models Propagate Race-Based Medicine},
  author       = {Omiye, Jesutofunmi A. and Lester, Jenna C. and Spichak, Simon and Rotemberg, Veronica and Daneshjou, Roxana},
  date         = {2023-10-20},
  journaltitle = {npj Digital Medicine},
  shortjournal = {npj Digit. Med.},
  volume       = {6},
  number       = {1},
  pages        = {195},
  issn         = {2398-6352},
  doi          = {10.1038/s41746-023-00939-z},
  url          = {https://www.nature.com/articles/s41746-023-00939-z},
  urldate      = {2025-06-28},
  abstract     = {Abstract             Large language models (LLMs) are being integrated into healthcare systems; but these models may recapitulate harmful, race-based medicine. The objective of this study is to assess whether four commercially available large language models (LLMs) propagate harmful, inaccurate, race-based content when responding to eight different scenarios that check for race-based medicine or widespread misconceptions around race. Questions were derived from discussions among four physician experts and prior work on race-based medical misconceptions believed by medical trainees. We assessed four large language models with nine different questions that were interrogated five times each with a total of 45 responses per model. All models had examples of perpetuating race-based medicine in their responses. Models were not always consistent in their responses when asked the same question repeatedly. LLMs are being proposed for use in the healthcare setting, with some models already connecting to electronic health record systems. However, this study shows that based on our findings, these LLMs could potentially cause harm by perpetuating debunked, racist ideas.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/SZQDQFYY/Omiye et al. - 2023 - Large language models propagate race-based medicin.pdf}
}

@article{Trust,
  title        = {Influence of Believed {{AI}} Involvement on the Perception of Digital Medical Advice},
  author       = {Reis, Moritz and Reis, Florian and Kunde, Wilfried},
  date         = {2024-11},
  journaltitle = {Nature Medicine},
  shortjournal = {Nat Med},
  volume       = {30},
  number       = {11},
  pages        = {3098--3100},
  issn         = {1078-8956, 1546-170X},
  doi          = {10.1038/s41591-024-03180-7},
  url          = {https://www.nature.com/articles/s41591-024-03180-7},
  urldate      = {2025-06-28},
  abstract     = {Abstract                            Large language models offer novel opportunities to seek digital medical advice. While previous research primarily addressed the performance of such artificial intelligence (AI)-based tools, public perception of these advancements received little attention. In two preregistered studies (               n               \,=\,2,280), we presented participants with scenarios of patients obtaining medical advice. All participants received identical information, but we manipulated the putative source of this advice (‘AI’, ‘human physician’, ‘human\,+\,AI’). ‘AI’- and ‘human\,+\,AI’-labeled advice was evaluated as significantly less reliable and less empathetic compared with ‘human’-labeled advice. Moreover, participants indicated lower willingness to follow the advice when AI was believed to be involved in advice generation. Our findings point toward an anti-AI bias when receiving digital medical advice, even when AI is supposedly supervised by physicians. Given the tremendous potential of AI for medicine, elucidating ways to counteract this bias should be an important objective of future research.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/HSIWIPI7/Reis et al. - 2024 - Influence of believed AI involvement on the percep.pdf}
}

@article{Advice,
  title        = {Influence of Believed {{AI}} Involvement on the Perception of Digital Medical Advice},
  author       = {Reis, Moritz and Reis, Florian and Kunde, Wilfried},
  date         = {2024-11},
  journaltitle = {Nature Medicine},
  shortjournal = {Nat Med},
  volume       = {30},
  number       = {11},
  pages        = {3098--3100},
  issn         = {1078-8956, 1546-170X},
  doi          = {10.1038/s41591-024-03180-7},
  url          = {https://www.nature.com/articles/s41591-024-03180-7},
  urldate      = {2025-06-28},
  abstract     = {Abstract                            Large language models offer novel opportunities to seek digital medical advice. While previous research primarily addressed the performance of such artificial intelligence (AI)-based tools, public perception of these advancements received little attention. In two preregistered studies (               n               \,=\,2,280), we presented participants with scenarios of patients obtaining medical advice. All participants received identical information, but we manipulated the putative source of this advice (‘AI’, ‘human physician’, ‘human\,+\,AI’). ‘AI’- and ‘human\,+\,AI’-labeled advice was evaluated as significantly less reliable and less empathetic compared with ‘human’-labeled advice. Moreover, participants indicated lower willingness to follow the advice when AI was believed to be involved in advice generation. Our findings point toward an anti-AI bias when receiving digital medical advice, even when AI is supposedly supervised by physicians. Given the tremendous potential of AI for medicine, elucidating ways to counteract this bias should be an important objective of future research.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/8A9PIRU4/Reis et al. - 2024 - Influence of believed AI involvement on the percep.pdf}
}

@article{EUAIAct,
  title        = {The {{EU Artificial Intelligence Act}} (2024): {{Implications}} for Healthcare},
  shorttitle   = {The {{EU Artificial Intelligence Act}} (2024)},
  author       = {Van Kolfschooten, Hannah and Van Oirschot, Janneke},
  date         = {2024-11},
  journaltitle = {Health Policy},
  shortjournal = {Health Policy},
  volume       = {149},
  pages        = {105152},
  issn         = {01688510},
  doi          = {10.1016/j.healthpol.2024.105152},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S0168851024001623},
  urldate      = {2025-06-28},
  abstract     = {In August 2024, the EU Artificial Intelligence Act (AI Act) entered into force. This legally binding instrument sets rules for the development, the placing on the market, the putting into service, and the use of AI systems in the European Union. As the world’s first extensive legal framework on AI, it aims to boost innovation while pro­ tecting individuals against the harms of AI. Since healthcare is one of the top sectors for AI deployment, the new rules will significantly reform national policies and practices on health technology. In this article, we highlight the implications of the AI Act for the healthcare sector. We give a comprehensive overview of the new legal obligations for various healthcare stakeholders (tech developers; healthcare professionals; public health au­ thorities). We conclude that, due to its horizontal approach, it is necessary to adopt further guidelines to address the unique needs of the healthcare sector. To this end, we make recommendations for the upcoming imple­ mentation and standardization phase.},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/RPZU5ZPA/Van Kolfschooten und Van Oirschot - 2024 - The EU Artificial Intelligence Act (2024) Implica.pdf}
}

@article{Privacy2,
  title        = {Privacy Preserving Strategies for Electronic Health Records in the Era of Large Language Models},
  author       = {Jonnagaddala, Jitendra and Wong, Zoie Shui-Yee},
  date         = {2025-01-16},
  journaltitle = {npj Digital Medicine},
  shortjournal = {npj Digit. Med.},
  volume       = {8},
  number       = {1},
  pages        = {34},
  issn         = {2398-6352},
  doi          = {10.1038/s41746-025-01429-0},
  url          = {https://www.nature.com/articles/s41746-025-01429-0},
  urldate      = {2025-06-28},
  langid       = {english},
  file         = {/Users/choekyelnyungmartsang/Zotero/storage/EBGEJ9PM/Jonnagaddala und Wong - 2025 - Privacy preserving strategies for electronic healt.pdf}
}
