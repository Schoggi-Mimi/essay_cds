\documentclass[12pt,a4paper]{scrreprt}
\usepackage{setspace}
\onehalfspacing

\usepackage[a4paper,
            top=2cm,
            bottom=2cm,
            left=2.5cm,
            right=2.5cm]{geometry}

% (Possibly remove if you're on a modern TeX system)
\usepackage[utf8]{inputenc}

\usepackage[T1]{fontenc}
\usepackage[english]{babel} % If needed

% Fonts
% \usepackage[scaled]{helvet} % for Helvetica
% \renewcommand{\familydefault}{\sfdefault} % Helvetica as default

\usepackage{mathptmx} % Times New Roman equivalent for text and math

% KOMA/hacks
\usepackage{scrhack}
%\usepackage{geometry} % Use either geometry OR the KOMA options above

% Tables
\usepackage{array}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{xcolor,colortbl}

% Lists
\usepackage{enumitem}

% Graphics
\usepackage{graphicx}
\usepackage[export]{adjustbox}

% Subfigures, PDF inclusion
\usepackage{subcaption}
\usepackage{pdfpages}

% Abstract
\usepackage{abstract}

% Table of contents custom
\usepackage{tocloft}

% Math & Symbols
\usepackage{amsmath,amssymb}
\usepackage{siunitx}
\usepackage{pifont}

% Language/quotations
\usepackage{csquotes}

% Page rotation
\usepackage{pdflscape}

% Positioning text blocks
\usepackage[absolute]{textpos}

% Time/date
\usepackage{datetime}

% BibLaTeX with Vancouver style
\usepackage[
  backend=biber,
  style=numeric-comp,
  sorting=none,
  citestyle=numeric
]{biblatex}
\addbibresource{references.bib}

% Hyperref last-ish
\usepackage{xurl}
\usepackage[
  breaklinks,
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue
]{hyperref}
\renewcommand{\chapterautorefname}{Chapter}
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Subsection}

% Optional: microtype for better typography
\usepackage{microtype}

% Fancy headers/footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\headsep}{0.5cm}
\fancyfoot[C]{\thepage\hspace{1.5cm}}

% Additional
\usepackage{comment}

% Custom column width
\newlength{\gridimagewidth}
\setlength{\gridimagewidth}{3cm}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}

\begin{document}

% Title Page
\begin{titlepage}
	\begin{textblock*}{5cm}[0,0](6.6cm,11.0cm)
	\end{textblock*}
	\begin{center}
		\vspace*{2cm}
    \Huge{\textbf{Essay:}}\\
		\Huge{\textbf{Trustworthy Large Language Models for Clinical Decision Support}}\\
    \vspace{3em}
		\Large{Clinical Decision Support}\\
    \vspace{0.5em}
    \normalsize Master of Science in Artificial Intelligence in Medicine\\
    \normalsize University of Bern\\
		\vspace{10em}
        \Large{Carla Paloma Malo Nivon}\\
        \vspace{0.5em}
        \normalsize Matriculation number: \\
        \vspace{4em}
        \Large{Choekyel Nyungmartsang}\\
        \vspace{0.5em}
        \normalsize Matriculation number: 21-876-693\\
		\vfill
        \Large{\today}\\

    
	\end{center}
 
\newpage

\thispagestyle{empty}
\begin{flushright}
    \includegraphics[width=5.5cm]{university_logo.png} % Adjust width as needed
\end{flushright}
\vspace{2cm}
\section*{Eigenständigkeitserklärung}
{\small
Hiermit versichern wir, dass wir die Arbeit mit dem Titel

\vspace{1cm}
\noindent
\rule{\textwidth}{0.5pt}
\noindent
eigenständig und nur mit den angegebenen Hilfsmitteln verfasst haben. Alle Texte, derer wir uns bedient haben, inklusive Quellen aus dem Internet, haben wir im Literaturverzeichnis aufgeführt. Wörtliche Übernahmen von Textpassagen Anderer haben wir als wörtliche Zitate gekennzeichnet. Wir haben auch die paraphrasierende Wiedergabe oder freie Übernahme fremder Gedanken durch Verweis auf einen Originaltext belegt. Falls wir Werkzeuge und Dienste der künstlichen Intelligenz eingesetzt haben oder die Arbeit von anderen Menschen haben gegenlesen lassen, haben wir dies in der Arbeit ausgewiesen. Wir versichern, dass die vorliegende Arbeit trotz Einsatz der etwaig aufgeführten Hilfsmittel und Unterstützungen im Wesentlichen unsere eigene Leistung ist.

\par
\vspace{\baselineskip}
\noindent
Uns ist bewusst, dass ein Verstoss gegen diese Vorgaben den Regeln der guten wissenschaftlichen Praxis widerspricht und ein Plagiat darstellt, das nach Richtlinien der Universität Bern geahndet werden kann.

\par
\vspace{\baselineskip}
\noindent
Damit diese und andere Arbeiten auf Plagiate überprüft werden können, erklären wir uns damit einverstanden, dass die Universität Bern diese Arbeit mit einer Plagiatssoftware überprüft und in einer Datenbank speichert, mit deren Hilfe zukünftige Arbeiten auf Plagiate überprüft werden können.

\vspace{1.5cm}
\noindent
\begin{tabularx}{\textwidth}{@{}lX}
	&\\
	Bern, \today &  \\
	\cline{2-2}
\end{tabularx}

\vspace{1.5cm}
\noindent
\begin{tabularx}{\textwidth}{@{}lX}
	&\\
	Bern, \today &  \\
	\cline{2-2}
\end{tabularx}
}
\pagenumbering{gobble}
\end{titlepage}

% Start of Questions Section on Page 2
\newpage
\cleardoublepage
\pagenumbering{arabic}
\setcounter{page}{1}
% Text.\autocite{citationkey}
% \par
% \vspace{\baselineskip}
% \noindent
In 2015 a team taught pigeons to spot breast-cancer patterns on pathology slides. After only two weeks the birds reached 85 \% accuracy, and when their answers were combined they matched expert pathologists \autocite{Pigeon}. If pigeons can learn patterns from pictures, machines that read huge amounts of text should do even better.\par
\vspace{\baselineskip}
\noindent
Large Language Models (LLMs) are deep neural networks that read billions of sentences and learn to predict the next word. They can draft notes, answer questions, and explain lab results in clear language. Because most clinical work is written, researchers now test LLMs as helpers in medical decisions. Early systems such as Med-PaLM, GatorTron, and ChatGPT have already passed medical exams and can summarise electronic health records within minutes.\par
\vspace{\baselineskip}
\noindent
Interest is high, but medicine is demanding. Wrong or biased advice can harm patients, leak private data, and damage public trust. This essay asks a simple question: what must change before LLMs can be trusted for clinical decision support? The next sections describe the current technology, examine five key barriers and then suggest practical ways to move ahead.

\section*{Overview of current state of technology}
LLMs have advanced quickly since the transformer design made large-scale text learning practical. Today these models range from a few million to more than five hundred billion parameters, and many receive extra training on medical text to build clinical skill \autocite{Survey}.\par
\vspace{\baselineskip}
\noindent
The first wave of general models was led by ChatGPT, trained on broad web data. It drew attention in medicine when it scored at or near the pass mark on all three steps of the United States Medical Licensing Exam even without special medical tuning \autocite{ChatGPT}. Although the test used sample questions, the result showed that a text model can recall much of the knowledge doctors learn in school.\par
\vspace{\baselineskip}
\noindent
Researchers soon built health-specific models. Google’s Med-PaLM and the newer Med-PaLM 2, trained on large sets of medical questions, reached 67 \% and then 86 \% accuracy on USMLE-style tests, becoming the first to beat the human pass threshold with room to spare \autocite{MedPalm}. In work with clinical notes, GatorTron grew to 8.9 billion parameters and learned from ninety billion words of anonymised electronic health records, improving performance on five main language tasks such as concept extraction and inference \autocite{LLM}.\par
\vspace{\baselineskip}
\noindent
To compare these systems fairly, teams created MultiMedQA, a benchmark that blends six exam and consumer datasets with new web health queries. Human reviewers then score answers for factual correctness, reasoning, and possible harm, exposing weaknesses that simple accuracy numbers can miss \autocite{LLM2}.

\section*{Ethical barriers to trustworthy LLMs}
Research shows that language models can pass medical exams, but real-world care still reveals serious risks. The five issues that follow hallucinations, bias, privacy leaks, lack of explanation, and loss of public trust, separate current early systems from safe daily use. Each subsection sets out the problem, offers evidence, and explains why it matters for patients and clinicians.

\subsection*{A. Hallucinations and factual errors}
LLMs can speak with full confidence while inventing facts. In medicine this flaw is called a hallucination. A recent study in NEJM AI asked leading models to assign ICD-10 codes to real patient notes and found that even GPT-4 gave the exact correct code in fewer than half of the cases, while many outputs were imprecise or made-up \autocite{AINotReady2024,LLM3}.\par
\vspace{\baselineskip}
\noindent
In routine care such slips are not minor quirks. A non-existent drug or wrong dose can delay treatment or cause an overdose, and incorrect codes bend billing records and research data. The load falls on clinicians, who must read every AI line but stay responsible if a hidden mistake appears in the record. Ethically this breaks the rule of “do no harm” and weakens informed consent because neither doctor nor patient can tell when the system is guessing. Until hallucinations drop to a level close to human error and models can flag uncertainty in plain language, their output cannot be trusted as a standalone source for clinical decisions.

\subsection*{B. Cognitive bias and healthcare disparities}
LLMs learn from the text they read. If that text carries old race-based ideas, the model can copy them. A 2023 Nature study asked four popular models questions about kidney function, lung capacity, and pain. All four at times told users to adjust care by race or claimed that Black patients feel pain differently, even though these views have been disproved \autocite{Race}.\par
\vspace{\baselineskip}
\noindent
When a model repeats bias, the harm can spread quickly. Skewed dose or triage advice may steer doctors toward the wrong test or delay care for people who already face barriers. Biased cost or prognosis estimates can also push health plans and policy makers to unfair choices. These mistakes break the duty of justice, raise the risk of unequal treatment, and weaken trust among patients who have seen bias before.


\subsection*{C. Privacy and data security}
Language models keep traces of the text they saw during training. If that text includes real patient notes, skilled users can pull private facts back out. A 2024 Nature study showed that even after names and dates were removed, an attacker could ask short probe questions and learn whether a patient’s record had been in the training set \autocite{Privacy}.\par
\vspace{\baselineskip}
\noindent
Leaks break laws like the General Data Protection Regulation (GDPR) and the Health Insurance Portability and Accountability Act (HIPAA) and can harm patients far beyond the clinic. A revealed HIV status or rare-disease note could hurt a person’s job or insurance prospects. Fear of such leaks may also stop hospitals from sharing data that is needed to train fairer models. Technical fixes such as local hospital deployment, differential-privacy noise, and strict logging are under study, but none fully block the risk yet \autocite{Privacy2}. Until clear guardrails show that no sensitive text can escape, privacy worries will limit how far LLMs can move into everyday clinical work.


\subsection*{D. Lack of explainability}
Most language models can give an answer but cannot show how they reached it. Apple researchers tested new “reasoning” models and saw them solve easy puzzles, then fail on harder ones while still speaking with full confidence. They called this gap the “illusion of thinking” and showed that longer chains of thought did not lead to better logic \autocite{Apple}.\par
\vspace{\baselineskip}
\noindent
In medicine this lack of clarity is a problem. Doctors must explain to patients and colleagues why they order a test or change a drug dose. If the model offers only a smooth answer without clear evidence, the clinician has little ground to defend the choice. Hidden logic also blocks safety checks because reviewers cannot trace where an error began. This weakens accountability and leaves both patients and providers unsure whether to follow the advice. Until models can show reliable, easy-to-read reasons for each clinical step, their guidance will stay outside the main decision process.


\subsection*{E. Erosion of trust in clinical AI overall}
Bad news spreads quickly. A 2024 Nature Medicine survey asked people how much they trust doctors who use AI support. Many said they prefer a doctor who works without any AI, and headlines about failed tools were the main reason \autocite{Trust}. When a well-known system fails, people start to doubt every AI tool. Each public failure makes it harder for newer, safer models to gain acceptance. Good systems may never reach patients if the public sees all clinical AI as risky. Fixing this will need strict testing, open sharing of errors, and clear proof that new language models avoid the mistakes of the old ones.


\section*{Implications for key stakeholders}
Clinicians are the first to handle LLM output, and patients depend on the advice that follows. Responsibility for care still rests with the human professional, so every AI suggestion needs a quick but careful review. The extra time spent checking can extend visits, and patients notice the delay. A 2024 Nature Medicine survey found that many people trust medical advice less once they learn an AI helped, even when the answer is correct \autocite{Advice}. When hallucinations, bias, or data leaks slip through, trust can collapse and workloads can rise instead of fall.\par
\vspace{\baselineskip}
\noindent
Regulators are acting in parallel. The EU AI Act puts clinical decision support in the high-risk category, which means outside audits, full trace records, and clear proof of safety are required before launch \autocite{EUAIAct}. For developers this means privacy by design, clear uncertainty flags, and public test reports are now expected. Without these steps, public concern can keep even the best tools out of daily use.

\section*{Conclusion}
LLMs will not earn a place in clinical care until five gaps close: fewer hallucinations, regular bias checks, strong privacy protection, clear explanations, and open error reports. Each fix is within reach. Benchmarks such as MultiMedQA now measure reasoning and potential harm, giving teams a clear target. The EU AI Act already lists decision support as high risk, so builders must show safety data before release. Local deployment, retrieval-augmented design, and simple uncertainty flags are reducing mistakes in trial units.\par
\vspace{\baselineskip}
\noindent
Progress is steady, but trust will depend on proof from daily practice. Strong audits, shared lessons, and plain-language reports can show patients and doctors that these tools are safe. And while pigeons will never roam hospital halls, their success at spotting cancer reminds us how powerful pattern learning can be. With the right safeguards, language models can become reliable partners for clinicians and their patients.




\par
\vspace{\baselineskip}
\noindent

% References Section
% \newpage
\printbibliography

\end{document}